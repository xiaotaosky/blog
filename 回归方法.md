<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## 一、线性回归
## 1.1基本理论
线性回归模型假设样本的特征（自变量X）与因变量（Y）之间存在线性关系。给一个随机样本$$(Y_i,X_{i1},...,X_{i}p),i=1,...,n$$，误差项设为随机变量$$\varepsilon_i$$，线性回归模型可以表示为：    
<div align="center">$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + ... + \beta_p X_{ip} + \varepsilon_i$$</div>  
损失函数为Least square，即$$L(\beta) = \frac{1}{n}\sum{(y_i-\hat{y_i})^2} = \frac{1}{n}\|y-X\hat{\beta} \|^2$$，当使得损失函数最小时，  
<div align="center">$$\hat{\beta} = (X^T X)^{-1}X^Ty，\hat{y} = X\hat{\beta}$$</div>  

## 1.2置信区间
我们用$$\delta^2$$代表误差项$$\varepsilon$$的方差，可以证明$$\delta^2$$有如下的一个无偏估计（其中$$S=\sum\hat{\varepsilon}_i^2$$是误差（残差）平方和）：
<div align="center">$$\hat{\delta}^2=\frac{S}{n-p-1}$$</div>  
估计值和实际值之间的关系是（即自由度为n-p-1的卡方分布）：
<div align="center">$$\hat{\delta}^2*\frac{n-p-1}{\delta^2}\sim\chi_{n-p-1}^2$$</div>
如果所观察的误差服从正态分布，则
<div align="center">$$\hat{\beta}\sim N(\beta,\delta^2(X^TX)^{-1}$$</div>  
残差估计值
<div align="center">$$\hat{\delta_j}=\sqrt{\frac{S}{n-p-1}[(X^TX)^{-1}]_{jj}}$$</div>    
参数$$\hat{\beta}$$的$$100(1-\alpha)%$$置信区间为$$\hat{\beta}_j \pm t_{\frac{\alpha}{2},n-p-1}\hat{\delta}_j$$，误差项表示为$$\hat{r}=y-X\hat{\beta}$$。

## 1.3拟合优度检验与统计量
拟合优度检验模型对样本观察值的拟合程度，其方法是构造一个统计量，与某一标准比较，得出结论。  
> 离差平方和$$SST=\sum(y_i - \bar{y})^2$$，值越大，则观察值离散程度（波动）越大，自由度为n-1。  
回归平方和$$SSR=\sum(\hat(y_i) - \bar{y})^2$$，自由度为n-p-1。  
残差平方和$$SSE=\sum(y_i-\hat{y_i})$$，自由度为p。  
SST = SSR + SSE。一个拟合得比较好的模型，SST与SSR比较接近，而SSE尽可能小，于是可以将SST与SSR的表达式作为拟合优度检验的统计量。  

###### 1、相关系数  
p=2时,$$r = \sqrt{\frac{SSR}{SST}} = \frac{L_{xy}}{\sqrt{L_{xx}L_{yy}}} = \frac{\sum{(x_i-\bar{x})(y_i-\bar{y})}}{\sqrt{\sum(x_i-\bar{x})^2\sum(y_i-\bar{y})^2}} $$  
当$$\|r\| -> 0$$，X与Y之间的线性关系越不明显，不适合使用线性回归模型，反之，$$\|r\| $$越接近1，线性关系越密切。  
一般情况下，$$R^2 = 1 - \frac{SSR/(n-p-1)}{SST/(n-1)}$$，其值越大，拟合效果越好。  

###### 2、F统计量  
假设$$H_0:\beta_1,\beta_2,...,\beta_p全部为0$$，构造统计量  
<div align="center">$$F=\frac{SSR/(p*\delta^2)}{SSE/((n-p-1)*\delta^2)} = \frac{(n-p-1)*SSR}{p* SSE} \sim F(p,n-p-1)$$</div>  
F值越大，整体的拟合效果越好。给定检验水平$$\alpha$$，检验标准为：当用样本计算的$$F>F_\alpha(p,n-p-1)$$，则拒绝$$H_0$$，意味着Y与X存在回归关系，否则，接受假设，回归关系不存在。  
&emsp;&emsp;F与$$R^2$$存在等量关系，它们对整体的拟合效果分析是一致的。只是$$R^2$$是一个统计指标，不是通过“提出假设、统计推断、检验决策”的假设检验过程得出来的。  
###### 3、t统计量  
F检验是检验整体回归拟合程度，t检验是检验某个特征的拟合程度。假设$$H_0:\beta_i = 0$$。
构造统计量  
<div align="center">$$t=\frac{\hat{\beta_i}}{\hat{\delta_i}} \sim t(n-p-1)$$</div>  
其中$$\hat{\delta_i}$$为估计量$$\hat{\beta_i}$$的标准差的估计量。

## 二、岭回归
&emsp;&emsp;在线性回归最小二乘方法中，参数的求解可能出现“病态矩阵”。$$\hat{\beta} = (X^T X)^{-1}X^Ty$$中的$$X^TX$$可能不满秩，或者接近于奇异，这样导致求解逆$$(X^T X)^{-1}$$时误差很大，为解决这个问题，可以加入一个正则项，即在原来的Square Least损失函数的基础上，加上一个正则（L2正则），目标函数为：  
<div align="center">$$min. \|y-X^T\beta\|^2 + \lambda \|\beta\|^2$$</div>  
&emsp;&emsp;最后求解得，  
<div align="center">$$\hat{\beta} = (X^TX + \lambda I)^{-1}X^Ty$$</div>  
&emsp;&emsp;岭回归是对最小二乘的补充，它损失了无偏性，提高了数值的稳定性，从而提升计算精度。  


## 三、Logistic回归
&emsp;&emsp;在二分类问题中，设分类标签为0和1，我们考虑用样本值去拟合类别为1的概率，即用$$X_{i1},X_{i2},...,X_{ip}$$拟合$$p(x)=P(Y=1|X=x)$$，最简单最直接的方法是考虑用线性拟合。但是$$\omega X$$是无边界的，而p(x)在0和1之间，所以不能直接拟合。如果考虑线性拟合logp(x)，由于logp(x)是单侧有界的，所以也行不通。最后最简单的办法是考虑去拟合$$log \frac{p(x)}{1-p(x)}$$，这样就不用考虑$$\beta X$$没有意义。  
&emsp;&emsp;Logistic回归模型为：  
<div align="center">$$log \frac{p(x)}{1-p(x)}=\omega x$$</div>   
<div align="center">$$p(x;\omega)=\frac{1}{1+e^{ -\omega x}}$$</div>    
###### 极大似然函数与问题求解  
由于Logistic是拟合概率，所以可以使用极大似然估计来求$$\omega$$。总体的密度函数为$$f(x,y)=\begin{cases}P(Y=1|X=x)&y=1\\1-P(Y=1|X=x)&y=0 \end{cases}=p(x)^y(1-p(x))^{1-y}$$  
似然函数为  
<div align="center">$$L(\omega)=\prod p(x_i)^y_i(1-p(x_i))^{1-y_i}$$</div>    
&emsp;&emsp;求最大值时，我们不直接求$$\frac{\partial L(\omega)}{\partial \omega} = 0$$，而是使用Newton 方法。在Newton逼近方法中，求f(x) = 0 时，通过$$x_{n+1} = x_n - \frac{f(x_n)}{f(x_n)^\prime}$$。  
&emsp;&emsp;求似然函数最大值时，其迭代为  
<div align="center">$$\omega_{n+1} = \omega_n - \frac{f(\omega_n)^\prime}{f(\omega_n)^{\prime \prime}}$$</div>    

