#一、基础模型
    支持向量机（SVM）是一个有监督的二分类模型，其策略是在特征空间上寻找使类分隔间隔最大的线性分类器。
     https://xiaotaosky.github.io/blog/svm-1.png
设分类器为：$\omega^T X + b = 0$，而两个类别的边界分割线为：$\omega^T X + b = \pm1$，将类别标记设为$y_i\in\{1,-1\}$。
于是，两个类别之间的最近距离为$\frac{2}{||\omega||^2}$，问题转化为：$max\frac{2}{||\omega||^2}$ ，$s.t. y_i( \omega^T x_i + b)\ge1$，它等价于：$$min\frac{||\omega||^2}{2}$ ，$s.t. y_i( \omega^T x_i + b)\ge1$$

以上是假设分类问题是完全线性可分的，但有些情况下，数据可能会有噪声，对模型影响很大，例如下图中，由于被圈的蓝色的点的影响，导致使分类距离很宽的分类器变成使分类较窄的分类器。有时，甚至分类器无法线性可分。

对于这种情况，给每个数据一个松弛变量，允许$x_i$偏离分割线一定的值，当然，期望偏离越小越好，于是，目标函数为：
$$min\frac{||\omega||^2}{2}+C\sum{\xi_i}$ ，$s.t. y_i( \omega^T x_i + b)\ge1-\xi_i$$

#二、Hinge Loss与SVM
分类问题的目标函数大多可以转化为损失函数与正则项的和。
Hinge Loss定义为 $H = max{0,1-ty}，t=\pm1，y=\omega^T x$，对于分类正确的点，H=0。
在约束条件 $y_i( \omega^T x_i + b)\ge1-\xi_i$下，如果Hinge Loss添加正则项$\frac{||\omega||^2}{2}$，那么目标函数为：
$$min\{C \sum{H(i)}+\frac{||\omega||^2}{2} \} = $min\{\frac{||\omega||^2}{2}+C\sum{\xi_i}\}$$
与SVM一致。于是SVM即为Hinge Loss下的二正则项约束模型。

#三、模型求解
对于问题： $min\frac{||\omega||^2}{2}$ ，$s.t. y_i( \omega^T x_i + b)\ge1$，可以等价于：
 $$\min_ {\omega,b} \max_{\alpha_i \geq 0} L(\omega,b,\alpha) = \frac{1}{2} ||\omega||^2 + \sum_{i=0}^n\alpha_i(y_i(\omega^Tx_i+b)-1)$$
其原因是在满足条件下，$$\max_{\alpha_i \geq 0} L(\omega,b,\alpha) = \frac{1}{2} ||\omega||^2$$
又由于 $$\min_ {\omega,b} \max_{\alpha_i \geq 0} L(\omega,b,\alpha) \geq \max_{\alpha_i \geq 0}\min_ {\omega,b} L(\omega,b,\alpha)$$,
所以，固化$\alpha$，由拉格朗日乘子法得，
$$\frac{\partial{L}}{\partial{\omega}}=0 \Rightarrow \omega=\sum_{i=1}^n \alpha_i y_i x_i，\frac{\partial{L}}{\partial{b}}=0 \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0$$
问题转化为
$$\max_{\alpha_i }   L(\omega,b,\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1} ^n \alpha_i\alpha_j y_i y_j x_i^T x_j，s.t.,\alpha_i \geq 0 ,\sum_{i=1}^n \alpha_i y_i = 0$$
在数据不太干净，引入松弛变量的情况下，也可以有类似的推导：目标函数不变，约束条件为$C \geq \alpha_i \geq 0 ,\sum \alpha_i y_i = 0$。
使用序列最小优化算法SMO求解拉格朗日对偶问题中的$\alpha$。首先选取一对参数$(\alpha_i,\alpha_j)$，固定其余参数的值，将$(\alpha_i,\alpha_j)$代入到优化问题中，求最优的更新后的$(\alpha_i,\alpha_j)$，不断重复直到收敛。
尽管每次选取的有两个变量$(\alpha_i,\alpha_j)$，但是在约束条件$\sum \alpha_i y_i = 0$的条件下，求最优解的问题事实上是一个单变量的规划问题，目标函数是关于$\alpha_i$的二次函数，约束条件是$\alpha_i>0$，很容易求出满足最值时的$\alpha_i$，于是，将这个过程反复迭代，最终至收敛。

#四、核函数
引入核函数的目的是将数据从低维特征空间映射到高维特征空间，是原本不能线性可分的数据变成线性可分。常见的核函数有：
1、线性核函数：$K(x_i,x_j) = (x_i,x_j)$
2、多项式核函数：$K(x_i,x_j) = ((x_i,x_j)+1)^d $
3、径向基核函数RBF（高斯核函数）：$K(x_i,x_j) = exp\{-\frac{||x_i-x_j||^2}{\delta^2}\}$
4、拉普拉斯核函数：$K(x_i,x_j) = exp\{-\frac{||x_i-x_j||}{\delta}\}$
应用核函数时，将$x_i^T x_j$替换成$K(x_i,x_j)$.

#五、优缺点
SVM泛化错误率低，计算开销不大，结果容易解释；但是，对参数调节和核函数的选择敏感，原始模型仅适合处理二类问题。

#六、示例

#参考链接：
https://www.zhihu.com/question/40546280?sort=created
http://blog.csdn.net/shijing_0214/article/details/51017029
http://www.dataguru.cn/thread-371987-1-1.html
https://zhuanlan.zhihu.com/p/21932911?refer=baina





