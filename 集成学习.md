<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

# 集成学习

## Bagging  
&emsp;&emsp;Bagging是通过多个弱分类器生成一个强强分类器。在原始样本集中有放回的随机采样，形成一组训练集，在每个训练机上训练分类器，然后再将这些分类器组合起来。  

## 随机森林  
&emsp;&emsp;随机森林生成每棵子树时，训练集是通过有放回抽样得到的，训练集的数据量与原训练集数据量相同；  
&emsp;&emsp;每棵子树分裂时，并不是选用所有的待选特征，而是从待选特征中随机选取一定的特征，然后再从中选择最优特征进行分裂。  
&emsp;&emsp;最后，子树分类结果的众数就是随机森林分类的结果。  

## Boosting  
&emsp;&emsp;Boosting也是通过弱分类器生成强分类器，但不是像Bagging那样通过不同的训练集合生成弱分类器。首先，设所有的样本的权重相等，训练一个分类器后，将错误划分的样本的权重提高，在训练下一个分类器时使用这个权重信息，直到生成一个高精度的分类器。  

## AdaBoost  
<div ><img src="https://xiaotaosky.github.io/blog/集成学习-1.png"/></div>  

## Boosting Decision Tree提升树算法  
<div ><img src="https://xiaotaosky.github.io/blog/集成学习-2.png"/></div>   

## GBDT梯度提升决策树  
&emsp;&emsp;GBDT是GB（梯度提升）和DT（Regression Decision Tree回归树）的结合，GB是指沿梯度方向构造一系列弱分类器(base weak learner)，并以一定的权重组合形成最终的强分类器。  


###### GB
&emsp;&emsp;设样本为$${x_i,y_i},i=1,2,...,n$$。首先初始化一个常量预测函数$$\gamma，F_0(x)=arg min_{\gamma}L(y_i,\gamma)$$。  
接下来按照如下规则迭代M步，生成其余的预测函数：  
&emsp;&emsp;在第m步中，$$F_0,F_1,...,F_{m-1}$$已经在前面的步骤中生成，接下来，求误差对预测函数的梯度$$r_{im}=\frac{\partial{L(y_i,F(x_i))}}{\partial{F(x_i)}}|_{F(x)=F_{m-1}}$$，  
&emsp;&emsp;然后在训练集合$$(x_i,r_{im})$$上生成一个预测模型（例如，回归树）$$h_m(x)$$，更新$$F_m(x)=F_{m-1}(x)+\gamma_mh_m(x)$$，其中，$$\gamma_m=argmin_{\gamma}\sum_{i=1}^{n}L(y_i,F_{m-1}(x_i)+\gamma h_m(x))$$。  

###### DT  
&emsp;&emsp;这里的决策树是回归树不是分类树。回归树的输出不是分类，而且预测值。在递归过程，计算每个特征A的每个分割点s对应的最小均方误差，每次取最小误差对应的分割点，计算最小均方误差时，可以求得各个划分子树的预测值，当递归达到结束条件时，各个子树的预测值是回归树的输出。  
<div align="center"><img width="300" height="300" src="https://xiaotaosky.github.io/blog/集成学习-3.png"/></div>   
参考：http://www.jianshu.com/p/005a4e6ac775  

## XGBoost  
&emsp;&emsp;在GBDT中，每次迭代都是通过在梯度方向生成一棵树去逼近最终的决策树（类似于牛顿逼近）。在xgboost中，每次迭代时使用整体误差作为目标函数去训练新的分类器，并且还增加了正则项防止模型过拟合。  

1、首先，给出树的定义，$$f(x_i)=w_{q(x_i)}，  
其中，x_i落在第q(x_i)个叶子节点，该叶子节点的取值为w_{q(x_i)}$$  
2、在训练一棵树T时，目标函数构造如下$$Obj(f)=L(f)+\Omega (f)$$，其中$$L(\theta)$$是损失函数，$$\Omega(\theta)$$是正则项，  
$$\Omega(\theta)=\gamma T + \frac{1}{2}\lambda \sum_{i=1}^{T}{w_i^2}$$，T 是树的复杂度。  
3、生成弱分类器迭代时，有  
<div align="center">$$\hat{y_i}^{t}=\sum_{r=1}^{t}{f_r(x_i)} =\hat{y_i}^{t-1} + f_t(x_i)$$</div>  
即保留上一次迭代生成的分类器，用然后去修正它。  
&emsp;&emsp;第t步的目标函数为  
<div align="center">$$Obj(\sum_{r=1}^{t}{f_r(x)})=L(\hat{y}^{t-1} + f_t(x))+\Omega (\sum_{r=1}^{t}{f_r(x)})$$</div>  
&emsp;&emsp;在第t步时，$$f_1(x),f_2(x),...,f_{t-1}(x)$$均是已知的,又由于  
&emsp;&emsp;$$f(x+\Delta x) \approx f(x) + f'(x)\Delta x + \frac{1}{2}f''(x)\Delta x^2$$</div>  
于是， $$Obj^t = \sum_{i=1}^{n}[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)] + \Omega(f_t) + constant，其中g_i=L'(\hat{y}_i^{t-1})，h_i=L''(\hat{y}_i^{t-1})$$
将树代入有，  
&emsp;&emsp;$$Obj^t = \sum_{i=1}^{T}[G_i\omega_i + \frac{1}{2}(H_i+\lambda)\omega_i^2] + \gamma T</div>  
其中G_i、H_i是第i个叶子的所有x值对应的g_i、h_i的和$$
&emsp;&emsp;于是，其最小值为：
<div ><img src="https://xiaotaosky.github.io/blog/集成学习.png"/></div>  
   
4、求树的结构  
在第t步中，我们求得了最小的目标函数的值。在接下来一步中，在已经生成的树的基础上做新的修正，于是，需要选择某个特征做分割， 
<div ><img src="https://xiaotaosky.github.io/blog/集成学习-4.png"/></div>   
于是，算法总结如下：  
<div ><img src="https://xiaotaosky.github.io/blog/集成学习-5.png"/></div>  
更加详细的内容见：http://djjowfy.com/2017/08/01/XGBoost的原理/

